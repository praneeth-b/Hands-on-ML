{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    # The code to download the mnist data original came from\n",
    "    # https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html\n",
    "    \n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    try: \n",
    "        from urllib.request import urlretrieve \n",
    "    except ImportError: \n",
    "        from urllib import urlretrieve\n",
    "\n",
    "    def load_data(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x3080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if n != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
    "                    )\n",
    "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if crow != 28 or ccol != 28:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
    "                    )\n",
    "                # Read data.\n",
    "                res = np.frombuffer(\n",
    "                    gz.read(num_samples * crow * ccol), dtype=np.uint8\n",
    "                )\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples, crow, ccol)) / 256\n",
    "\n",
    "\n",
    "    def load_labels(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x1080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))\n",
    "                if n[0] != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
    "                    )\n",
    "                # Read labels.\n",
    "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples))\n",
    "\n",
    "\n",
    "    def try_download(data_source, label_source, num_samples):\n",
    "        data = load_data(data_source, num_samples)\n",
    "        labels = load_labels(label_source, num_samples)\n",
    "        return data, labels\n",
    "    \n",
    "    \n",
    "    # Not sure why, but yann lecun's website does no longer support \n",
    "    # simple downloader. (e.g. urlretrieve and wget fail, while curl work)\n",
    "    # Since not everyone has linux, use a mirror from uni server.\n",
    "    #     server = 'http://yann.lecun.com/exdb/mnist'\n",
    "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master'\n",
    "    \n",
    "    # URLs for the train image and label data\n",
    "    url_train_image = f'{server}/train-images-idx3-ubyte.gz'\n",
    "    url_train_labels = f'{server}/train-labels-idx1-ubyte.gz'\n",
    "    num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data\")\n",
    "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "    # URLs for the test image and label data\n",
    "    url_test_image = f'{server}/t10k-images-idx3-ubyte.gz'\n",
    "    url_test_labels = f'{server}/t10k-labels-idx1-ubyte.gz'\n",
    "    num_test_samples = 10000\n",
    "\n",
    "    print(\"Downloading test data\")\n",
    "    test_features, test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading test data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, test_features, test_labels = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "from tqdm import tqdm \n",
    "\n",
    "class ImgGenerator:\n",
    "    def __init__(self, orig_img, orig_label):\n",
    "        self.orig_img = orig_img\n",
    "        self.orig_label = orig_label\n",
    "        self.generated_imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def shift_image(self,img, x,y):\n",
    "        shifted_img = shift(img,[x,y])\n",
    "        return shifted_img\n",
    "    \n",
    "    \n",
    "    def generate_im(self, shift_x, shift_y):\n",
    "        pb = tqdm(total=len(self.orig_label))\n",
    "        for img, label in zip(self.orig_img, self.orig_label):\n",
    "            right_shift = self.shift_image(img, 0, shift_x)#.reshape(-1,28*28)\n",
    "            left_shift  = self.shift_image(img, 0, -shift_x)#.reshape(-1,28*28)\n",
    "            up_shift = self.shift_image(img, shift_y,0)#.reshape(-1,28*28)\n",
    "            down_shift = self.shift_image(img, -shift_y,0)#.reshape(-1,28*28)\n",
    "            self.generated_imgs.extend([img, right_shift, left_shift, up_shift, down_shift])\n",
    "            self.labels.extend([label]*5)\n",
    "            pb.update(1)\n",
    "        return self.generated_imgs, self.labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 59904/60000 [00:26<00:00, 2229.29it/s]"
     ]
    }
   ],
   "source": [
    "# #use the generated images to train the ANN\n",
    "p = ImgGenerator(train_features, train_labels)\n",
    "gen_img, gl = p.generate_im(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_features[9])\n",
    "print('Label: {}'.format(test_labels[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(x, y, frac):\n",
    "    lx = len(x)\n",
    "    p = np.random.permutation(len(x))\n",
    "    \n",
    "    return x[p[:int(lx*frac)]], y[p[:int(lx*frac)]], x[p[int(lx*frac):]], y[p[int(lx*frac):]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember:\n",
    "> With reshape you can stack all pixels in a big vector that can be used as NN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features1 = train_features.reshape(-1, 28*28)\n",
    "test_features1 = test_features.reshape(-1, 28*28)\n",
    "print(train_features1.shape, train_labels.shape)\n",
    "#x_val, y_val, train_features1, train_labels = validation_split(train_features1, train_labels, 0.1)\n",
    "\n",
    "#gen_img = np.array(gen_img)\n",
    "#gen_img1 = gen_img.reshape(-1,28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, operation):\n",
    "        self.value = np.array(value)\n",
    "        self.operation = operation\n",
    "        \n",
    "    \n",
    "#     def backprop(self):\n",
    "#         # We define the backpropagation code later.\n",
    "#         return backprop(self)\n",
    "    \n",
    "class Parameter(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used for Variables that are learnable.\n",
    "    You can later use this class to distinguish learnable variables\n",
    "    from other variables (`isinstance(variable, Parameter)`).\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        self.gradient = np.zeros_like(self.value)\n",
    "        \n",
    "class Input(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used as wrapper for inputs that are not learnable.\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self,D):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_param(self, values):\n",
    "        param = Parameter(values)\n",
    "        self.parameters.append(param)\n",
    "        return param\n",
    "    \n",
    "    def update_parameters(self, optimizer):\n",
    "        for param in self.parameters:\n",
    "            optimizer.update(param)\n",
    "        \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        small_value = 0.01\n",
    "        weight_vals = np.random.uniform(\n",
    "                    size=[in_units, out_units],\n",
    "                    low=-small_value,\n",
    "                    high=small_value\n",
    "                    )\n",
    "        self.W = self.add_param(weight_vals)\n",
    "        self.b = self.add_param(np.zeros(shape=out_units))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        def backward(D):\n",
    "            #print(self.__class__.__name__)\n",
    "            \n",
    "            self.W.gradient = self.W.gradient + X.T @ D  \n",
    "                                  ## todo to check for correctness (is += required)\n",
    "            self.b.gradient = self.b.gradient + np.sum(D,axis=0) \n",
    "           \n",
    "            return D @ self.W.value.T\n",
    "       \n",
    "       \n",
    "        return X @ self.W.value + self.b.value, backward\n",
    "    \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for layer in layers:\n",
    "            self.parameters.extend(layer.parameters) \n",
    "        \n",
    "    def forward(self, X):\n",
    "        backprops = []\n",
    "        op = X\n",
    "        for layer in self.layers:\n",
    "            op, backprop = layer.forward(op)\n",
    "            backprops.append(backprop)\n",
    "\n",
    "        def backward(D):\n",
    "            for backprop in reversed(backprops):\n",
    "                D = backprop(D)\n",
    "            return D\n",
    "        return op , backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, X):\n",
    "        mask = X > 0\n",
    "        return X * mask, lambda D: D * mask\n",
    "        \n",
    "    \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, X):\n",
    "        op = 1/(1+np.exp(-X))\n",
    "        \n",
    "        def backward(D):\n",
    "            return D #* op * (1 - op)\n",
    "        \n",
    "        return op, backward\n",
    "    \n",
    "class SGDOptimizer():\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, param):\n",
    "        param.value = param.value - self.lr * param.gradient \n",
    "        param.gradient.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-11):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions))/N\n",
    "    return ce , predictions - targets   #todo verify derivative of ce loss\n",
    "\n",
    "\n",
    "\n",
    "def mse_loss(Y_, Y):\n",
    "    diff = Y_ - Y.reshape(Y_.shape)\n",
    "    return np.square(diff).mean(), 2 * diff / len(diff)\n",
    "\n",
    "    \n",
    "\n",
    "def one_hot_encoder(x_label):\n",
    "    rows = x_label.shape[0]\n",
    "    oh_x = np.zeros((rows, 10))\n",
    "    for i in range(rows):\n",
    "        oh_x[i][x_label[i]] = 1\n",
    "    \n",
    "    return oh_x\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self,X):\n",
    "        exps = np.exp(X - np.max(X))\n",
    "        def backward(D):\n",
    "            return D\n",
    "        return exps / np.sum(exps), backward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt = one_hot_encoder(np.array([3,5]))\n",
    "# pred = np.random.rand(2,10)\n",
    "# l,d = cross_entropy(pred,tgt)\n",
    "# tgt*pred\n",
    "\n",
    "# print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DigitLearner():\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def fit_batch(self,X,Y):\n",
    "        Y_, backward = self.model.forward(X)\n",
    "          #softmax_crossentropy_with_logits\n",
    "        \n",
    "        L , D = self.loss(Y_, Y)           ## todo loss function derivative \n",
    "                                \n",
    "     \n",
    "        backward(D)\n",
    "        self.model.update_parameters(self.optimizer)\n",
    "        return L\n",
    "  \n",
    "    def fit(self, X, Y, epochs, bs):\n",
    "        losses = []\n",
    "        val_loss=[]\n",
    "        pbar = tqdm(total=epochs)\n",
    "        for epoch in range(epochs):\n",
    "            p = np.random.permutation(len(X))\n",
    "            L = 0\n",
    "            VL = 0\n",
    "            for i in range(0, len(X), bs):\n",
    "                X_batch = X[p[i:i + bs]]\n",
    "                Y_batch = Y[p[i:i + bs]]\n",
    "                L += self.fit_batch(X_batch, Y_batch)\n",
    "                \n",
    "  \n",
    "            losses.append(L)\n",
    "\n",
    "            pbar.update(1)\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, xtest):\n",
    "        ypred, _ = self.model.forward(xtest)\n",
    "        return ypred.argmax(axis=-1)\n",
    "    \n",
    "    def accuracy(y_pred, y_test):\n",
    "        return np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "    \n",
    "    def dump_params(self,filename):\n",
    "        with open(filename, 'w') as fp:\n",
    "            json.dump([p.value.tolist() for p in self.model.parameters],fp)\n",
    "            print(\"weights written to file\")\n",
    "            \n",
    "    def load_params(self, filename):\n",
    "        with open(filename) as fp:\n",
    "            loaded_params = json.load(fp)\n",
    "        for p, p_value in zip(self.model.parameters, loaded_params):\n",
    "            p.value = p_value\n",
    "        print(\"Loaded given weights!!\")\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_parameters(loss, file):\n",
    "    variables = get_variables(J)\n",
    "    parameters = [\n",
    "        v \n",
    "        for v in variables\n",
    "        if isinstance(v, Parameter)\n",
    "    ]\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump([p.value.tolist() for p in parameters], fp)\n",
    "    print('Wrote the parameters')\n",
    "    print(parameters)\n",
    "    print('to', file)\n",
    "\n",
    "#dump_parameters(loss=J, file='parameters.json')\n",
    "\n",
    "\n",
    "def load_parameters(loss, file):\n",
    "    variables = get_variables(J)\n",
    "    parameters = [\n",
    "        v \n",
    "        for v in variables\n",
    "        if isinstance(v, Parameter)\n",
    "    ]\n",
    "    with open(file) as fp:\n",
    "        parameters_values = json.load(fp)\n",
    "    for p, p_value in zip(parameters, parameters_values):\n",
    "        print(p.value, p_value)\n",
    "        p.value[...] = p_value\n",
    "        \n",
    "    print('Loaded the parameters')\n",
    "    print(parameters)\n",
    "    print('from', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 256\n",
    "lrate = 0.01\n",
    "epochs = 35\n",
    "batch = 32\n",
    "X = train_features1 #gen_img1\n",
    "y = train_labels\n",
    "Y = one_hot_encoder(train_labels)  # np.array(gl)  train_labels\n",
    "\n",
    "\n",
    "X_test = test_features1\n",
    "Y_test = test_labels\n",
    "\n",
    "test = DigitLearner(\n",
    "    Sequential(\n",
    "        AffineLayer(784, hidden_neurons), \n",
    "        ReLU(), \n",
    "        AffineLayer(hidden_neurons, hidden_neurons),\n",
    "        ReLU(),\n",
    "        AffineLayer(hidden_neurons, 10),\n",
    "        Sigmoid()        \n",
    "    ), \n",
    "    cross_entropy, \n",
    "    SGDOptimizer(lr=lrate)\n",
    ")\n",
    "\n",
    "losses = test.fit(X, Y, epochs=epochs, bs=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to a json file\n",
    "test.dump_params(\"wt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new empty model here to load weights\n",
    "hidden_neurons = 256\n",
    "lrate = 0.01\n",
    "\n",
    "mt_model = DigitLearner(\n",
    "    Sequential(\n",
    "        AffineLayer(784, hidden_neurons), \n",
    "        ReLU(), \n",
    "        AffineLayer(hidden_neurons, hidden_neurons),\n",
    "        ReLU(),\n",
    "        AffineLayer(hidden_neurons, 10),\n",
    "        Sigmoid()        \n",
    "    ), \n",
    "    cross_entropy, \n",
    "    SGDOptimizer(lr=lrate)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights from a saved json file in a new model\n",
    "mt_model.load_params(\"wt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new empty model with weights loaded from json to make prediction\n",
    "tacc = DigitLearner.accuracy(mt_model.predict(X_test), Y_test)\n",
    "print(tacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(losses)\n",
    "plt.plot(losses)\n",
    "\n",
    "plt.show()\n",
    "acc = DigitLearner.accuracy(test.predict(X_test), Y_test)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test.predict(X_test[:10])\n",
    "print(pred, Y_test[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
